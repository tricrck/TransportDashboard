"""Update db

Revision ID: f9d8a5ae450a
Revises: 
Create Date: 2026-01-16 17:55:09.957766

"""
from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision = 'f9d8a5ae450a'
down_revision = None
branch_labels = None
depends_on = None


def upgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('data_validation_logs',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('data_source_id', sa.Integer(), nullable=False),
    sa.Column('status', sa.String(length=20), nullable=False),
    sa.Column('validated_at', sa.DateTime(), nullable=False),
    sa.Column('total_records', sa.Integer(), nullable=True),
    sa.Column('validation_errors', sa.JSON(), nullable=True),
    sa.ForeignKeyConstraint(['data_source_id'], ['data_sources.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    with op.batch_alter_table('data_validation_logs', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('ix_data_validation_logs_data_source_id'), ['data_source_id'], unique=False)

    with op.batch_alter_table('data_sources', schema=None) as batch_op:
        batch_op.add_column(sa.Column('api_timeout', sa.Integer(), nullable=True))
        batch_op.add_column(sa.Column('file_url', sa.String(length=1000), nullable=True))
        batch_op.add_column(sa.Column('file_hash', sa.String(length=64), nullable=True))
        batch_op.add_column(sa.Column('db_type', sa.String(length=50), nullable=True))
        batch_op.add_column(sa.Column('db_host', sa.String(length=200), nullable=True))
        batch_op.add_column(sa.Column('db_port', sa.Integer(), nullable=True))
        batch_op.add_column(sa.Column('db_name', sa.String(length=200), nullable=True))
        batch_op.add_column(sa.Column('db_username', sa.String(length=200), nullable=True))
        batch_op.add_column(sa.Column('db_password_encrypted', sa.Text(), nullable=True))
        batch_op.add_column(sa.Column('schema', sa.JSON(), nullable=True))
        batch_op.add_column(sa.Column('schema_inferred_at', sa.DateTime(), nullable=True))
        batch_op.add_column(sa.Column('column_count', sa.Integer(), nullable=True))
        batch_op.add_column(sa.Column('sample_data', sa.JSON(), nullable=True))
        batch_op.add_column(sa.Column('validation_enabled', sa.Boolean(), nullable=True))
        batch_op.add_column(sa.Column('last_validation_status', sa.String(length=20), nullable=True))
        batch_op.add_column(sa.Column('last_validation_errors', sa.JSON(), nullable=True))
        batch_op.add_column(sa.Column('refresh_in_progress', sa.Boolean(), nullable=True))
        batch_op.add_column(sa.Column('celery_task_id', sa.String(length=255), nullable=True))
        batch_op.add_column(sa.Column('cache_key', sa.String(length=255), nullable=True))
        batch_op.add_column(sa.Column('data_updated_at', sa.DateTime(), nullable=True))
        batch_op.add_column(sa.Column('min_response_time', sa.Float(), nullable=True))
        batch_op.add_column(sa.Column('max_response_time', sa.Float(), nullable=True))
        batch_op.add_column(sa.Column('consecutive_failures', sa.Integer(), nullable=True))
        batch_op.add_column(sa.Column('last_error_message', sa.Text(), nullable=True))
        batch_op.add_column(sa.Column('alert_on_failure', sa.Boolean(), nullable=True))
        batch_op.add_column(sa.Column('alert_threshold', sa.Integer(), nullable=True))
        batch_op.add_column(sa.Column('alert_sent_at', sa.DateTime(), nullable=True))
        batch_op.add_column(sa.Column('requires_approval', sa.Boolean(), nullable=True))
        batch_op.add_column(sa.Column('deleted_at', sa.DateTime(), nullable=True))
        batch_op.alter_column('detected_format',
               existing_type=sa.VARCHAR(length=50),
               type_=sa.Enum('JSON', 'CSV', 'XML', 'EXCEL', 'PARQUET', 'PDF', 'DOCX', 'TXT', 'HTML', name='dataformat'),
               existing_nullable=True)
        batch_op.alter_column('file_size',
               existing_type=sa.INTEGER(),
               type_=sa.BigInteger(),
               existing_nullable=True)
        batch_op.alter_column('data_size_bytes',
               existing_type=sa.INTEGER(),
               type_=sa.BigInteger(),
               existing_nullable=True)
        batch_op.create_index(batch_op.f('ix_data_sources_cache_key'), ['cache_key'], unique=False)
        batch_op.create_index(batch_op.f('ix_data_sources_health_status'), ['health_status'], unique=False)
        batch_op.create_index(batch_op.f('ix_data_sources_reference'), ['reference'], unique=True)

    # ### end Alembic commands ###


def downgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    with op.batch_alter_table('data_sources', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('ix_data_sources_reference'))
        batch_op.drop_index(batch_op.f('ix_data_sources_health_status'))
        batch_op.drop_index(batch_op.f('ix_data_sources_cache_key'))
        batch_op.alter_column('data_size_bytes',
               existing_type=sa.BigInteger(),
               type_=sa.INTEGER(),
               existing_nullable=True)
        batch_op.alter_column('file_size',
               existing_type=sa.BigInteger(),
               type_=sa.INTEGER(),
               existing_nullable=True)
        batch_op.alter_column('detected_format',
               existing_type=sa.Enum('JSON', 'CSV', 'XML', 'EXCEL', 'PARQUET', 'PDF', 'DOCX', 'TXT', 'HTML', name='dataformat'),
               type_=sa.VARCHAR(length=50),
               existing_nullable=True)
        batch_op.drop_column('deleted_at')
        batch_op.drop_column('requires_approval')
        batch_op.drop_column('alert_sent_at')
        batch_op.drop_column('alert_threshold')
        batch_op.drop_column('alert_on_failure')
        batch_op.drop_column('last_error_message')
        batch_op.drop_column('consecutive_failures')
        batch_op.drop_column('max_response_time')
        batch_op.drop_column('min_response_time')
        batch_op.drop_column('data_updated_at')
        batch_op.drop_column('cache_key')
        batch_op.drop_column('celery_task_id')
        batch_op.drop_column('refresh_in_progress')
        batch_op.drop_column('last_validation_errors')
        batch_op.drop_column('last_validation_status')
        batch_op.drop_column('validation_enabled')
        batch_op.drop_column('sample_data')
        batch_op.drop_column('column_count')
        batch_op.drop_column('schema_inferred_at')
        batch_op.drop_column('schema')
        batch_op.drop_column('db_password_encrypted')
        batch_op.drop_column('db_username')
        batch_op.drop_column('db_name')
        batch_op.drop_column('db_port')
        batch_op.drop_column('db_host')
        batch_op.drop_column('db_type')
        batch_op.drop_column('file_hash')
        batch_op.drop_column('file_url')
        batch_op.drop_column('api_timeout')

    with op.batch_alter_table('data_validation_logs', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('ix_data_validation_logs_data_source_id'))

    op.drop_table('data_validation_logs')
    # ### end Alembic commands ###
